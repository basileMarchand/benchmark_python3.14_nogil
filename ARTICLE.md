
# ğŸš€ Benchmarks de Python 3.14b2 avec `--disable-gil`

## ğŸ§  Un peu de contexte : le GIL, pourquoi et pour quoi faire ?

Le GIL (*Global Interpreter Lock*) est un mÃ©canisme prÃ©sent dans lâ€™implÃ©mentation CPython depuis toujours. Il garantit quâ€™un seul et unique thread peut exÃ©cuter du bytecode Python Ã  la fois. Ce mÃ©canisme permet historiquement de grandement simplifier la gestion de la mÃ©moire interne de lâ€™interprÃ©teur.

NÃ©anmoins, ce verrou global a un effet de bord majeur : **les programmes multi-threads ne scalent pas sur plusieurs cÅ“urs** pour des workloads *CPU-bound*.

â¡ï¸ En pratique, `threading.Thread` est donc surtout utilisÃ© pour des I/O. 

Depuis des annÃ©es, la suppression du GIL Ã©tait un sujet rÃ©current dans la communautÃ© Python. Depuis Python 3.13, Ã§a se concrÃ©tise !

## ğŸ§ª Python 3.13 introduit un mode expÃ©rimental : `--disable-gil`

Câ€™est lâ€™objet de la [PEP 703 â€“ Making the Global Interpreter Lock Optional](https://peps.python.org/pep-0703/) proposÃ©e par Sam Gross, qui a fait beaucoup parler dâ€™elle. Depuis Python 3.13, il est possible de permettre la dÃ©sactivation du GIL â€” pour cela, il faut que l'interprÃ©teur Python soit compilÃ© avec l'option `--disable-gil`.

Avec ce mode activÃ© Ã  la compilation, le GIL est pilotable via la variable d'environnement `PYTHON_GIL=0|1`. 

Par exemple, pour lancer un script en mode `nogil`, il suffit de faire : 

```bash
PYTHON_GIL=0 python3 my_script.py
```

Quelques benchmarks existent en ligne, par exemple [celui-ci](https://medium.com/@r_bilan/python-3-13-without-the-gil-a-game-changer-for-concurrency-5e035500f0da). Mais jâ€™ai voulu approfondir un peu le sujet et Ã©valuer l'apport potentiel de cette rÃ©volution !

## ğŸ”§ Setup pour les tests 

Pour faire mes tests, jâ€™ai mis en place une image Docker qui compile Python 3.14b2 avec l'option `--disable-gil`. Le Dockerfile et lâ€™ensemble des cas de test sont disponibles [ici](...).

Pour construire le Docker :

```
$ docker build . -f Dockerfile-nogil -t python:nogil_3.14b2
``` 

## âš™ï¸ Benchmark #1 : Factorielle lourde en multi-thread

Pour le premier test, on fait tout bÃªtement un gros calcul de factorielle dans plusieurs threads.

```python
import threading
import math
import time 

def test_function(num_thread):
    thread_start_time = time.time()
    math.factorial(250000)
    thread_execution_time = time.time() - thread_start_time

start_time = time.time()

threads = []
for num_thread in range(5):
    thread = threading.Thread(target=test_function, args=(num_thread,))
    thread.start()
    threads.append(thread)

for thread in threads:
    thread.join()

execution_time = time.time() - start_time
print(f"     Elapsed time: {execution_time:.2f} seconds")
```

### ğŸ§¾ RÃ©sultats

| Mode         | Temps total |
|--------------|-------------|
| GIL ON       | 1.97 s      |
| GIL OFF      | 0.56 s      |

âœ… **Gain Ã—3~4** : ici, `--disable-gil` permet une vraie parallÃ©lisation du calcul lourd avec un trÃ¨s bon scaling.

## âš™ï¸ Benchmark #2 : Calculs mathÃ©matiques modÃ©rÃ©s

MÃªme idÃ©e que prÃ©cÃ©demment, mais avec une charge mathÃ©matique un peu plus variÃ©e.

```python
import threading
import math
import time
import random

def stress_function(thread_id, complexity):
    result = 0
    for i in range(1, complexity):
        a = math.sqrt(i) + math.sin(i) ** 2
        b = math.log1p(i) * math.exp(-a)
        c = math.factorial(i % 500 + 500) % 10**8  # bornÃ© pour ne pas exploser la RAM
        result += a * b + c

def run_threads(num_threads=4, complexity=10_000):
    print(f"Launching {num_threads} threads with complexity {complexity}")
    threads = []

    start_time = time.time()
    for i in range(num_threads):
        t = threading.Thread(target=stress_function, args=(i, complexity))
        t.start()
        threads.append(t)

    for t in threads:
        t.join()
    total_time = time.time() - start_time
    print(f"   Elapsed time {total_time:.2f} seconds")

if __name__ == "__main__":
    run_threads(num_threads=5, complexity=20_000)
```

### ğŸ§¾ RÃ©sultats

| Mode         | Temps total |
|--------------|-------------|
| GIL ON       | 1.36 s      |
| GIL OFF      | 0.41 s      |

âœ… **Gain Ã—3~4 Ã  nouveau**, scaling constant.

## âš™ï¸ Benchmark #3 : AccÃ¨s concurrent partagÃ©

On ajoute ici des Ã©critures dans une variable partagÃ©e (une liste Python). Chaque thread Ã©crit dans une case distincte.

```python
def stress_function(thread_id, complexity, shared_data, lock):
    local_result = 0.0
    for i in range(1, complexity):
        a = math.sqrt(i) + math.sin(i) ** 2
        b = math.log1p(i) * math.exp(-a)
        c = math.factorial(i % 500 + 500) % 10**8
        local_result += a * b + c
    shared_data[thread_id] = local_result

def run_threads(num_threads=4, complexity=10_000):
    threads = []
    shared_data = [0.0] * num_threads
    lock = threading.Lock()

    start_time = time.time()
    for i in range(num_threads):
        t = threading.Thread(target=stress_function, args=(i, complexity, shared_data, lock))
        t.start()
        threads.append(t)

    for t in threads:
        t.join()

    total_time = time.time() - start_time
    total_sum = sum(shared_data)
    print(f"     Elapsed time: {total_time:.2f} seconds")
    print(f"        -> Global result: {total_sum % 1000:.2f}")
```

### ğŸ§¾ RÃ©sultats

| Mode         | Temps total |
|--------------|-------------|
| GIL ON       | 1.32 s      |
| GIL OFF      | 0.43 s      |

ğŸ”’ GrÃ¢ce Ã  un dÃ©coupage propre ğŸ˜ (pas de collision entre threads), `nogil` reste trÃ¨s efficace ici.

## âš ï¸ Benchmark #4 : Recherche du plus proche voisin sur 10M points

Test plus rÃ©aliste : 10M de points 3D alÃ©atoires, recherche du plus proche voisin en multithreading avec dictionnaire partagÃ©.

```python
import threading
import random
import time
from math import sqrt

def distance2(a, b):
    return (a[0]-b[0])**2 + (a[1]-b[1])**2 + (a[2]-b[2])**2


def find_closest_worker(points, query_point, start, end, shared_result, lock, tid):
    local_min_dist = float("inf")
    local_min_idx = -1

    for i in range(start, end):
        d = distance2(points[i], query_point)
        if d < local_min_dist:
            local_min_dist = d
            local_min_idx = i

    with lock:
        if local_min_dist < shared_result['min_dist']:
            shared_result['min_dist'] = local_min_dist
            shared_result['closest_idx'] = local_min_idx
            shared_result['owner'] = tid


def threaded_closest_point(points, query_point, num_threads=4):
    n = len(points)
    chunk_size = n // num_threads

    shared_result = {
        'min_dist': float("inf"),
        'closest_idx': -1,
        'owner': -1
    }
    lock = threading.Lock()
    threads = []

    for tid in range(num_threads):
        start = tid * chunk_size
        end = (tid + 1) * chunk_size if tid < num_threads - 1 else n
        t = threading.Thread(target=find_closest_worker,
                             args=(points, query_point, start, end, shared_result, lock, tid))
        t.start()
        threads.append(t)

    for t in threads:
        t.join()

    return shared_result['closest_idx'], sqrt(shared_result['min_dist']), shared_result['owner']

if __name__ == "__main__":
    N = 10_000_000
    num_threads = 4
    points = [[random.random(), random.random(), random.random()] for _ in range(N)]
    query_point = [random.random(), random.random(), random.random()]

    print(f"Launching threaded NN search with {num_threads} threads")
    t0 = time.time()
    idx, dist, owner = threaded_closest_point(points, query_point, num_threads)
    t1 = time.time()

    print(f"    Elapsed time: {t1 - t0:.2f} seconds")
```

### ğŸ§¾ RÃ©sultats

| Mode         | Temps total |
|--------------|-------------|
| GIL ON       | 2.38 s      |
| GIL OFF      | 3.61 s â—    |

âŒ Et lÃ , grosse surprise : activer `nogil` rend les choses plus lentes. Je n'ai pas vraiment d'explication certaine. Mais si je devais avancer une thÃ©orie je dirais que l'accÃ¨s concurrent Ã  la grosse liste Python entraine quelques limitations. 

## ğŸ” Bonus : version C++ Ã©quivalente + `nanobind`

Jâ€™ai aussi testÃ© ce cas en C++ multithreadÃ© natif, exposÃ© Ã  Python via [`nanobind`](https://github.com/wjakob/nanobind). c'est surtout pour voir si en suivant exactement la mÃªme logique mais en changeant juste la stack technique on obtient des rÃ©sultats diffÃ©rents. Et donc oui : 

| ImplÃ©mentation      | Temps total |
|---------------------|-------------|
| Python GIL ON       | 2.38 s      |
| Python GIL OFF      | 3.61 s â—    |
| C++ (threads)       | 0.88 s âœ…    |

En version threads c++ on a bien un gros gain de performance. Donc la logique est bonne, pas trop mauvaise en tout cas, la limitation vient de Python ğŸ˜¿. 

## ğŸ”š Conclusion

Python progresse â€” câ€™est indÃ©niable et enthousiasmant. Mais pour des cas fortement concurrentiels, les implÃ©mentations natives (C/C++) gardent un net avantage.


- âœ… Python 3.14 `--disable-gil` **offre des gains rÃ©els** pour des tÃ¢ches *CPU-bound* multithreadÃ©es
- âš ï¸ Mais les performances **ne sont pas systÃ©matiquement meilleures**, notamment dÃ¨s quâ€™il y a de gros volumes de donnÃ©es ou de la contention mÃ©moire

## ğŸ“š Pour aller plus loin

- [PEP 703 â€” Making the Global Interpreter Lock Optional](https://peps.python.org/pep-0703/)
- [Python 3.14 changelog (beta)](https://docs.python.org/3.14/whatsnew/3.14.html)
- [nanobind GitHub](https://github.com/wjakob/nanobind)